{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773bd51-0616-42be-acd8-2e5b4ad6953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalações necessárias:\n",
    "!pip install tqdm\n",
    "!pip install matplotlib\n",
    "!pip install efficientnet_pytorch\n",
    "!pip install flwr\n",
    "!pip install scikit-learn\n",
    "!pip install ray\n",
    "!pip install seaborn\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7c2b7-49db-4a26-af24-ba05f7dcb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "\n",
    "import flwr as fl\n",
    "from flwr .common import Metrics, Context\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torchvision.datasets import ImageFolder, CIFAR10\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ced13-7df7-4f8c-acb0-46e8653f0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "START = time.time\n",
    "LR = 0.001\n",
    "DATE_NOW = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "dataset = ImageFolder(\"./datasetBH\")\n",
    "basic_parameters = {\n",
    "'num_classes' : len(dataset.classes),\n",
    "'class_names': dataset.classes,\n",
    "'input_size' : 224,\n",
    "'batch_size' : 16,\n",
    "'lr' : 0.001, # Taxa de aprendizado\n",
    "'mm' : 0.9, # Momentum\n",
    "'epochs' : 50,\n",
    "'model_name' : \"alexnet\", # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "'criterion' : nn.CrossEntropyLoss(), # Função de perda\n",
    "'seed' : 42,\n",
    "'num_clients' : 3,\n",
    "'num_rounds' : 3,\n",
    "}\n",
    "\n",
    "MODEL = \"alexnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be0c709-1ced-474e-856b-3cfc635a05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_confusion_matrix(y_true, y_pred, class_names, output_dir, accuracy, loss,true_labels, predicted_labels, model_name=MODEL):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix\\nAccuracy: {accuracy:.2%}, Loss: {loss:.4f}')\n",
    "    \n",
    "    DATE_NOW = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Save the plot as a PDF\n",
    "    output_dir = output_dir + r'/outputs/' + model_name\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    result_dir = output_dir + '/' + model_name +'_' + DATE_NOW\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    output_path = os.path.join(result_dir, f'{model_name}_confusion_matrix.pdf')\n",
    "    plt.savefig(output_path, format=\"pdf\", bbox_inches='tight')\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    metrics_path = os.path.join(result_dir, \"metrics.txt\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy:.2%}\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1-score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Loss: {loss:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c347074-2822-495e-99c5-fd3f702f405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy_FED(val_losses, val_accs, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Plota gráfico de loss e accuracy por epoch para conjunto de treinamento e validação\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): Lista com os valores de loss do conjunto de treinamento por epoch\n",
    "        val_losses (list): Lista com os valores de loss do conjunto de validação por epoch\n",
    "        train_accs (list): Lista com os valores de accuracy do conjunto de treinamento por epoch\n",
    "        val_accs (list): Lista com os valores de accuracy do conjunto de validação por epoch\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define o número de epochs\n",
    "    epochs = len(val_losses)\n",
    "\n",
    "    # Define o eixo x do gráfico como o número de epochs\n",
    "    x = range(1, epochs + 1)\n",
    "\n",
    "    # Plota os gráficos de loss e accuracy para conjunto de treinamento e validação\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, val_losses, c='blue' ,ls='--', label='Val. loss', fillstyle='none')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, val_accs, c='blue' ,ls='--', label='Val. accuracy', fillstyle='none')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(save_dir, format=\"pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff3057-e715-4023-b2a7-ef7aaa402c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += predicted.tolist()\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a401aa8-b3e4-46d0-ad7e-e8d8e1844a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Class')\n",
    "    plt.xlabel('Predicted Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7898b-f7bc-4615-b657-048b2de70540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(train_losses, val_losses, train_accs, val_accs, model_name, fold, save_dir):\n",
    "    \"\"\"\n",
    "    Plota gráfico de loss e accuracy por epoch para conjunto de treinamento e validação\n",
    "\n",
    "    Args:\n",
    "        train_losses (list): Lista com os valores de loss do conjunto de treinamento por epoch\n",
    "        val_losses (list): Lista com os valores de loss do conjunto de validação por epoch\n",
    "        train_accs (list): Lista com os valores de accuracy do conjunto de treinamento por epoch\n",
    "        val_accs (list): Lista com os valores de accuracy do conjunto de validação por epoch\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define o número de epochs\n",
    "    epochs = len(train_losses)\n",
    "\n",
    "    # Define o eixo x do gráfico como o número de epochs\n",
    "    x = range(1, epochs + 1)\n",
    "\n",
    "    # Plota os gráficos de loss e accuracy para conjunto de treinamento e validação\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, train_losses, c='red' ,ls='-', label='Train loss', fillstyle='none')\n",
    "    plt.plot(x, val_losses, c='blue' ,ls='--', label='Val. loss', fillstyle='none')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, [acc.cpu() for acc in train_accs], c='red' ,ls='-', label='Train acuracy', fillstyle='none')\n",
    "    plt.plot(x, [acc.cpu() for acc in val_accs], c='blue' ,ls='--', label='Val. accuracy', fillstyle='none')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f'{save_dir}/{model_name}_fold_{fold}_loss_acc.pdf')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7acc1-14b8-452f-944e-c993fe9625f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report(model, dataloader,class_names, device='cpu'):\n",
    "    # Define o modelo para o dispositivo correto (CPU ou GPU)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Inicializa as variáveis de predições e rótulos verdadeiros\n",
    "    all_preds = torch.tensor([], dtype=torch.long, device=device)\n",
    "    all_labels = torch.tensor([], dtype=torch.long, device=device)\n",
    "\n",
    "    # Realiza a predição para cada lote de dados no dataloader\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # Adiciona as predições e rótulos verdadeiros às variáveis criadas anteriormente\n",
    "        all_preds = torch.cat((all_preds, preds), dim=0)\n",
    "        all_labels = torch.cat((all_labels, labels), dim=0)\n",
    "\n",
    "    # Gera o classification report com base nas predições e rótulos verdadeiros\n",
    "    report = metrics.classification_report(all_labels.cpu().numpy(), all_preds.cpu().numpy(),target_names=class_names,\n",
    "                                           digits=4, zero_division=0)\n",
    "\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d60eb-02fc-4fdf-99a3-33874ca2aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, basic_parameters, fold, date_now, device='gpu'):\n",
    "\n",
    "    # Tempo total do treinamento (treinamento e validação)\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    model_name = basic_parameters.get('model_name')\n",
    "    num_epochs = basic_parameters.get('epochs')\n",
    "    batch_size = basic_parameters.get('batch_size')\n",
    "\n",
    "    # Cria a pasta com o nome do modelo\n",
    "    output_dir = r'outputs/' + model_name\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Cria a pasta para separar os testes\n",
    "    result_dir = output_dir + '\\\\' + model_name +'_' + date_now\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    # Abre o arquivo para salvar o resultado\n",
    "    f = open(f'{result_dir}/{model_name}_fold_{fold}.txt', 'w')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        f.write(f'Epoch {epoch+1}/{num_epochs}\\n')\n",
    "        f.write('-' * 10 + '\\n')\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            # Inicia contagem de tempo da época\n",
    "            time_epoch_start = time.time()\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # Perda (loss) nesta época\n",
    "            running_loss = 0.0\n",
    "            # Amostras classificadas corretamente nesta época\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                model.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if model_name == 'inception' and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = basic_parameters.get('criterion')(outputs, labels)\n",
    "                        loss2 = basic_parameters.get('criterion')(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = basic_parameters.get('criterion')(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Atualiza a perda da época\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # Atualiza o número de amostras classificadas corretamente na época.\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            # Perda desta época\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            # Acurácia desta época\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            # Tempo total desta época\n",
    "            time_epoch = time.time() - time_epoch_start\n",
    "\n",
    "            f.write(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} ({time_epoch:.4f} seconds) \\n')\n",
    "\n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} ({time_epoch:.4f} seconds)')\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss_list.append(epoch_loss)\n",
    "                train_acc_list.append(epoch_acc)\n",
    "            else:\n",
    "                val_loss_list.append(epoch_loss)\n",
    "                val_acc_list.append(epoch_acc)\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_epoch = time.time() - since\n",
    "\n",
    "        f.write(f'Time: {time_epoch:.0f}s\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "        print(f'Time: {time_epoch:.0f}s')\n",
    "        print('\\n')\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    f.write(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\\n')\n",
    "    f.write(f'Number of epochs: {num_epochs}. Batch size: {batch_size}\\n')\n",
    "    f.write(f'Best val loss: {best_loss:.4f} Best val acc: {best_acc:.4f}\\n')\n",
    "\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val loss: {best_loss:.4f} Best val acc: {best_acc:.4f}')\n",
    "\n",
    "    # Save the confusion matrix\n",
    "    y_true, y_pred = evaluate_model(model, dataloaders['val'], device=device)\n",
    "    # Confusion matrix\n",
    "    conf_mat_val = metrics.confusion_matrix(y_true, y_pred)\n",
    "    f.write(f'\\nConfusion Matrix:\\n{conf_mat_val}\\n')\n",
    "\n",
    "    # print(f'Confusion Matrix:\\n{conf_mat_val}')\n",
    "\n",
    "    # Classification report\n",
    "    class_rep_val = generate_classification_report(model, dataloaders['val'],basic_parameters.get('class_names'), device)\n",
    "    f.write(f'\\nClassification report:\\n{class_rep_val}\\n')\n",
    "\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(conf_mat_val, classes=basic_parameters.get('class_names'))\n",
    "    plt.savefig(f'{result_dir}/{model_name}_fold_{fold}_cf_mat.pdf')\n",
    "\n",
    "    #Plota gráfico de loss e accuracy por epoch\n",
    "    plot_loss_accuracy(train_loss_list, val_loss_list, train_acc_list, val_acc_list, model_name, fold, result_dir)\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161199a5-ede1-4c61-8956-24bfd8ae60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, testloader, plot = False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.\n",
    "    true_labels = []\n",
    "    predicted_labels =  []\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Collect true and predicted labels for confusion matrix\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    if plot:    \n",
    "        result_dir = './outputs'\n",
    "        os.makedirs(result_dir, exist_ok=True)\n",
    "        save_confusion_matrix(true_labels, predicted_labels, CLASSES, result_dir, accuracy, loss, true_labels, predicted_labels)\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a27b2f-d346-4128-94ec-82728afba86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES = 2\n",
    "#CLASSES =  ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') #classes do CIFAR10\n",
    "CLASSES = ('benign', 'malignant') #Classes do BreakHis\n",
    "def load_data(basic_parameters, centralized = False):\n",
    "    # Load the breast cancer dataset (modify the paths accordingly)\n",
    "    data_transforms = {\n",
    "        'transform': transforms.Compose([\n",
    "\t\ttransforms.Resize([basic_parameters.get('input_size'), basic_parameters.get('input_size')], antialias=True),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\t    ]),\n",
    "        'teste': transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\t    ])\n",
    "    }\n",
    "\n",
    "    dataset = ImageFolder(\"./datasetBH\", transform=data_transforms['transform'])\n",
    "    #trainset = ImageFolder(\"./data/train\", transform=data_transforms['transform'])\n",
    "    #testset = ImageFolder(\"./data/test\", transform=data_transforms['transform'])\n",
    "    #return DataLoader(trainset, batch_size=basic_parameters.get('batch_size'), shuffle=True), DataLoader(testset)\n",
    "\n",
    "\n",
    "    # trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=data_transforms['transform'])\n",
    "    # testset = CIFAR10(\"./dataset\", train=False, download=True, transform=data_transforms['transform'])\n",
    "\n",
    "    #Split training set into 10 partitions to simulate the individual dataset\n",
    "    # partition_size = len(trainset) // basic_parameters.get('num_clients')\n",
    "    # lengths = [partition_size] * basic_parameters.get('num_clients')\n",
    "    if centralized or basic_parameters.get('num_clients') == 1: \n",
    "        print('Centralized dataset or Federated with only one client.')\n",
    "        #partition_size = len(trainset)\n",
    "        partition_size = len(dataset)\n",
    "        lengths = [partition_size] * 1\n",
    "        #datasets = random_split(trainset, lengths, torch.Generator().manual_seed(basic_parameters.get('seed')))\n",
    "        datasets = random_split(dataset, lengths, torch.Generator().manual_seed(basic_parameters.get('seed')))\n",
    "    else:\n",
    "        print(f\"Federated dataset with {basic_parameters.get('num_clients')} clients.\")\n",
    "        # partition_size = len(trainset) // basic_parameters.get('num_clients')\n",
    "        # remainder = len(trainset) % basic_parameters.get('num_clients')\n",
    "        partition_size = len(dataset) // basic_parameters.get('num_clients')\n",
    "        remainder = len(dataset) % basic_parameters.get('num_clients')\n",
    "        lengths = [partition_size] * (basic_parameters.get('num_clients') - 1) + [partition_size + remainder]\n",
    "        #datasets = random_split(trainset, lengths, torch.Generator().manual_seed(basic_parameters.get('seed')))\n",
    "        datasets = random_split(dataset, lengths, torch.Generator().manual_seed(basic_parameters.get('seed')))\n",
    "\n",
    "    #split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "\n",
    "    for idx, ds in enumerate(datasets, start=1):\n",
    "        print(f\"Client {idx}\")\n",
    "        print(f\"Dataset with {len(ds)} iamges.\")\n",
    "        #len_val = len(ds)//10 # 10% validation set\n",
    "        len_val = len(ds) // 5 # 20% validation set\n",
    "        len_train = len(ds)-len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        print(f\"Trainset with {len_train} iamges.\")\n",
    "        print(f\"Valset with {len_val} iamges.\")\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(basic_parameters.get('seed')))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=basic_parameters.get('batch_size'), shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=basic_parameters.get('batch_size')))\n",
    "    #testloaders = DataLoader(testset, batch_size=basic_parameters.get('batch_size'))\n",
    "\n",
    "\n",
    "    return trainloaders, valloaders#, testloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36417d1c-3177-4b2b-a62d-fabb196a15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "# \tdef __init__(self) -> None:\n",
    "# \t\tsuper(Net, self).__init__()\n",
    "# \t\tself.conv1 = nn.Conv2d(3,6,5)\n",
    "# \t\tself.pool=nn.MaxPool2d(2,2)\n",
    "# \t\tself.conv2 = nn.Conv2d(6, 16, 5)\n",
    "# \t\tself.fc1 = nn.Linear(16*5*5, 120)\n",
    "# \t\tself.fc2 = nn.Linear(120, 84)\n",
    "# \t\tself.fc3 = nn.Linear(84, 10)\n",
    "\t\n",
    "# \tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "# \t\tx = self.pool(F.relu(self.conv1(x)))\n",
    "# \t\tx = self.pool(F.relu(self.conv2(x)))\n",
    "# \t\tx = x.view(-1, 16 *5 *5)\n",
    "# \t\tx = F.relu(self.fc1(x))\n",
    "# \t\tx = F.relu(self.fc2(x))\n",
    "# \t\tx = self.fc3(x)\n",
    "# \t\treturn x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, basic_parameters=basic_parameters) -> None:\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        if basic_parameters.get('model_name') == \"alexnet\":\n",
    "            self.model = models.alexnet(weights='DEFAULT')\n",
    "            num_features = self.model.classifier[6].in_features\n",
    "            self.model.classifier[6] = nn.Linear(num_features, basic_parameters.get('num_classes'))\n",
    "        elif basic_parameters.get('model_name') == \"resnet\":\n",
    "            self.model = models.resnet50(weights='DEFAULT')\n",
    "            num_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(num_features, basic_parameters.get('num_classes')) \n",
    "        elif basic_parameters.get('model_name') == \"efficientnet\":\n",
    "            self.model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "            num_features = self.model._fc.in_features\n",
    "            self.model._fc = nn.Linear(num_features, basic_parameters.get('num_classes'))\n",
    "        elif basic_parameters.get('model_name') == \"squeezenet\":\n",
    "            self.model = models.squeezenet1_0(weights='DEFAULT')\n",
    "            self.model.classifier[1] = nn.Conv2d(512, basic_parameters.get('num_classes'), kernel_size=(1,1), stride=(1,1))\n",
    "            self.model.num_classes = basic_parameters.get('num_classes')    \n",
    "        elif basic_parameters.get('model_name') == \"vgg\": \n",
    "            self.model= models.vgg16(weights='DEFAULT')\n",
    "            num_ftrs = self.model.classifier[6].in_features\n",
    "            self.model.classifier[6] = nn.Linear(num_ftrs, basic_parameters.get('num_classes') )\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo não suportado: {basic_parameters.get('model_name')}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6349f683-53d9-4796-b78c-4c5682f5d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, trainloader, valloader, baisc_parameters, client):\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.basic_parameters = basic_parameters\n",
    "        self.client = client\n",
    "        self.dataloaders_dict = {'train': self.trainloader, 'val': self.valloader}        \n",
    "        self.optimizer = optim.SGD(self.net.parameters(), lr=self.basic_parameters.get('lr'), momentum=self.basic_parameters.get('mm'))\n",
    "        #self.optimizer = optim.Adam(self.net.parameters(), lr=self.basic_parameters.get('lr'))\n",
    "        self.date_now = None\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.date_now =  datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        #train(self.net, self.trainloader, epochs=self.basic_parameters.get('epochs'))\n",
    "        self.net = train_model(self.net, self.dataloaders_dict, self.optimizer, self.basic_parameters, self.client,self.date_now, DEVICE)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc65c85-1fd2-44be-b0b3-5e7140eaf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainloaders, valloaders, testloaders = load_data(basic_parameters)\n",
    "trainloaders, valloaders = load_data(basic_parameters)\n",
    "\n",
    "def client_fn(context: Context) -> FlowerClient:\n",
    "    net = Net().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "    num_partitions = context.node_config[\"num-partitions\"]\n",
    "    trainloader = trainloaders[int(partition_id)]\n",
    "    valloader = valloaders [int(partition_id)]\n",
    "    #testloader = testloaders\n",
    "    client = (1+int(partition_id))\n",
    "    return FlowerClient(net, trainloader, valloader, basic_parameters, client).to_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7901aa-5a74-4048-8170-b3c2ef573745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "  accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "  examples = [num_examples for num_examples, _ in metrics]\n",
    "  return {\"accuracy\": sum(accuracies)/sum(examples)}\n",
    "\n",
    "#Create FedAVG strategys\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=1.0, #sample 100% of available clients for training\n",
    "    fraction_evaluate=0.5, #sample 50% of available clients for evaluation\n",
    "    min_fit_clients=basic_parameters.get('num_clients'), #never sambple less than NUM_CLIENTS clients for training\n",
    "    min_evaluate_clients=int(basic_parameters.get('num_clients')/2), #never sample less than NUM_CLIENTS/2 clients for evaluation\n",
    "    min_available_clients=basic_parameters.get('num_clients'), #wait until all NUM_CLIENTS clients are available\n",
    "    evaluate_metrics_aggregation_fn=weighted_average,\n",
    ")\n",
    "#Start Simulation\n",
    "output = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients =basic_parameters.get('num_clients'),\n",
    "    config= fl.server.ServerConfig(num_rounds=basic_parameters.get('num_rounds')),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3d6ec-f4c1-4828-af78-af0fb6b3ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output FL - Loss: \", output.losses_distributed)\n",
    "print(\"Output FL - Accuracy: \", output.metrics_distributed[\"accuracy\"])\n",
    "\n",
    "result_dir = './outputs'\n",
    "fl_accuracy = []\n",
    "fl_loss = []\n",
    "f = open(f'{result_dir}/{basic_parameters.get('model_name')}acc_loss_rounds.txt', 'w')\n",
    "\n",
    "for loss in output.losses_distributed:\n",
    "      f.write(f\"Loss in Round {loss[0]}: {loss[1]}\")\n",
    "      print(f\"Loss in Round {loss[0]}: {loss[1]}\")\n",
    "      fl_loss.append(loss[1])\n",
    "      \n",
    "\n",
    "for acc in output.metrics_distributed[\"accuracy\"]:\n",
    "    \n",
    "      f.write(f\"Accuracy in Round {acc[0]}: {acc[1]}\")  \n",
    "      f.write('\\n')    \n",
    "      print(f\"Accuracy in Round {acc[0]}: {acc[1]}\")\n",
    "      fl_accuracy.append(acc[1])\n",
    "      \n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "x = range(len(fl_loss))\n",
    "ax.plot(x, fl_loss)\n",
    "plt.xlabel(\"Rounds\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = plt.axes()\n",
    "\n",
    "x = range(len(fl_accuracy))\n",
    "ax2.plot(x, fl_accuracy)\n",
    "plt.xlabel(\"Rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "\n",
    "f.close()\n",
    "\n",
    "# Save the plot as a PDF\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "DATE_NOW = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_path = os.path.join(result_dir, f'{DATE_NOW}_FL_{basic_parameters.get('model_name')}_loss_accuracy_plots.pdf')\n",
    "plot_loss_accuracy_FED(fl_loss, fl_accuracy, basic_parameters.get('model_name'), output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15489edb-af68-47bd-adbe-dade5d1b1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(\"./datasetBH\")\n",
    "basic_parameters = {\n",
    "'num_classes' : len(dataset.classes),\n",
    "'class_names': dataset.classes,\n",
    "'input_size' : 224,\n",
    "'batch_size' : 16,\n",
    "'lr' : 0.001, # Taxa de aprendizado\n",
    "'mm' : 0.9, # Momentum\n",
    "'epochs' : 450,\n",
    "'model_name' : \"alexnet\", # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "'criterion' : nn.CrossEntropyLoss(), # Função de perda\n",
    "'seed' : 42,\n",
    "'num_clients' : 3,\n",
    "'num_rounds' : 3,\n",
    "}\n",
    "\n",
    "#trainloaders, valloaders = load_data(basic_parameters, centralized=True)\n",
    "\n",
    "#model_ft = initialize_model(basic_parameters.get('model_name'), basic_parameters.get('num_classes'))\n",
    "model_ft = Net().to(DEVICE)\n",
    "optimizer = optim.SGD(model_ft.parameters(), lr=basic_parameters.get('lr'), momentum=basic_parameters.get('mm'))\n",
    "#optimizer = optim.Adam(model_ft.parameters(), lr=basic_parameters.get('lr'))\n",
    "date_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "\n",
    "#netCentralized = Net().to(DEVICE)\n",
    "trainloaders, valloaders = load_data(basic_parameters, centralized=True)\n",
    "trainloader = DataLoader(ConcatDataset([ds.dataset for ds in trainloaders]), batch_size=basic_parameters.get('batch_size'), shuffle=True)\n",
    "valloader = DataLoader(ConcatDataset([ds.dataset for ds in valloaders]), batch_size=basic_parameters.get('batch_size'))\n",
    "dataloaders_dict = {'train': trainloader, 'val': valloader}\n",
    "\n",
    "loss_per_epoch = []\n",
    "accuracy_per_epoch = []\n",
    "\n",
    "print(f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\")\n",
    "\n",
    "model_ft = train_model(model_ft, dataloaders_dict, optimizer, basic_parameters, 0,date_now, DEVICE)\n",
    "# for epoch in range(1000):\n",
    "# \ttrain(netCentralized, trainloader, 1)\n",
    "# \tloss, accuracy = test(netCentralized, valloader)\n",
    "# \tloss_per_epoch.append(loss)\n",
    "# \taccuracy_per_epoch.append(accuracy)\n",
    "# \tprint(f\"Epoch {epoch+1}: validation loss {loss}, accuracy: {accuracy}\")\n",
    "# train(netCentralized, trainloader, basic_parameters.get('num_clients'), valloader, True, True)\n",
    "\n",
    "# loss, accuracy = test(netCentralized, testloader, plot=True)\n",
    "# #loss_per_epoch.append(loss)\n",
    "# #accuracy_per_epoch.append(accuracy)\n",
    "# print(f\"Final test set performance: \\n\\tloss {loss} \\n\\taccuracy {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f63074-0485-4295-bd48-070ecd452183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
